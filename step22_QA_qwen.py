from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import pandas as pd
import torch

# === CONFIG ===
model_id = "Qwen/Qwen3-1.7B"
input_csv = "mimic_discharge_data.csv"
output_csv = "qa_pairs_qwen_generated.csv"
num_patients = 2  # You can scale this later

# === Load model ===
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", trust_remote_code=True)
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# === Prompt template ===
def build_prompt(discharge_summary):
    return f"""
You are a clinical NLP assistant.

Given the following discharge summary, generate:
1. A treatment-related factual question (e.g., why a drug was used)
2. A factual answer based on the summary
3. A counterfactual version of the question (negate the reason)
4. A reasonable counterfactual answer

Format:
Q1: <factual question>
A1: <factual answer>
Q2: <counterfactual question>
A2: <counterfactual answer>

Discharge Summary:
\"\"\"
{discharge_summary.strip()}
\"\"\"
"""

# === Process discharge summaries ===
df = pd.read_csv(input_csv)
df = df.head(num_patients)

results = []

for _, row in df.iterrows():
    subject_id = row["subject_id"]
    summary = row.get("discharge_summary", "")
    if not isinstance(summary, str) or len(summary.strip()) < 50:
        print(f"Skipping short or empty summary for subject {subject_id}")
        continue  # Skip short or empty notes

    prompt = build_prompt(summary)

    output = generator(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7)
    generated = output[0]['generated_text'][len(prompt):].strip()
    print(f"Generated for subject {subject_id}:\n{generated}\n")
    # Extract lines from LLM output
    lines = generated.split("\n")
    qa = {"subject_id": subject_id}
    for line in lines:
        if line.lower().startswith("q1"):
            qa["factual_question"] = line.split(":", 1)[-1].strip()
        elif line.lower().startswith("a1"):
            qa["factual_answer"] = line.split(":", 1)[-1].strip()
        elif line.lower().startswith("q2"):
            qa["counterfactual_question"] = line.split(":", 1)[-1].strip()
        elif line.lower().startswith("a2"):
            qa["counterfactual_answer"] = line.split(":", 1)[-1].strip()

    results.append(qa)

# print("results:", results)
# === Save results ===
qa_df = pd.DataFrame(results)
qa_df.to_csv(output_csv, index=False)
print(f"âœ… Saved {len(qa_df)} QA pairs generated by Qwen to: {output_csv}")
